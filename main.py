# -*- coding: utf-8 -*-
"""SU_21101_21124_21153.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AInQfqDbdIqY656SHcjk7zSFrcSD1P6w
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import csv
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

#this is our features for test
colnames_test = ['loan_amnt','term','grade','emp_length','home_ownership','annual_inc','tot_hi_cred_lim','verification_status','dti','pub_rec','tot_cur_bal','pct_tl_nvr_dlq','num_tl_op_past_12m','total_bal_ex_mort']
#read test data
test_data = pd.read_csv('loansTest.csv', usecols=colnames_test)
#pd.DataFrame(test_data).fillna(8)
#we used randomforestclassifier, so we need to convert numbers to strings.
test_data['term'] = test_data['term'].str.extract('(\d+)', expand=False)
test_data['emp_length'] = test_data['emp_length'].replace('< 1 year', 0.5)
test_data['emp_length'] = test_data['emp_length'].replace('1 year', 1)
test_data['emp_length'] = test_data['emp_length'].replace('2 years', 2)
test_data['emp_length'] = test_data['emp_length'].replace('3 years', 3)
test_data['emp_length'] = test_data['emp_length'].replace('4 years', 4)
test_data['emp_length'] = test_data['emp_length'].replace('5 years', 5)
test_data['emp_length'] = test_data['emp_length'].replace('6 years', 6)
test_data['emp_length'] = test_data['emp_length'].replace('7 years', 7)
test_data['emp_length'] = test_data['emp_length'].replace('8 years', 8)
test_data['emp_length'] = test_data['emp_length'].replace('9 years', 9)
test_data['emp_length'] = test_data['emp_length'].replace('10 years', 10)
test_data['emp_length'] = test_data['emp_length'].replace('10+ years', 10.5)

test_data['home_ownership'] = test_data['home_ownership'].replace('RENT', 1)
test_data['home_ownership'] = test_data['home_ownership'].replace('OWN', 2)
test_data['home_ownership'] = test_data['home_ownership'].replace('MORTGAGE', 3)
test_data['home_ownership'] = test_data['home_ownership'].replace('ANY',4)
test_data['home_ownership'] = test_data['home_ownership'].replace('OTHER',5)
test_data['home_ownership'] = test_data['home_ownership'].replace('NONE',6)

test_data['verification_status'] = test_data['verification_status'].replace('Verified',1)
test_data['verification_status'] = test_data['verification_status'].replace('Not Verified',2)
test_data['verification_status'] = test_data['verification_status'].replace('Source Verified',3)

test_data['grade'] = test_data['grade'].apply(ord)
# test dataframe which we readed on above.
test_data = pd.DataFrame({
    'loan_amnt' : test_data.loan_amnt.tolist(),
    'term' : test_data.term.tolist(),
    'grade' : test_data.grade.tolist(),
    'emp_length' : test_data.emp_length.tolist(),
    'home_ownership' : test_data.home_ownership.tolist(),
    'annual_inc' : test_data.annual_inc.tolist(),
    'verification_status' : test_data.verification_status.tolist(),
    'dti' : test_data.dti.tolist(),
    'pct_tl_nvr_dlq' : test_data.pct_tl_nvr_dlq.tolist(),
    'num_tl_op_past_12m' : test_data.num_tl_op_past_12m.tolist(),
    'pub_rec' : test_data.pub_rec.tolist(),
    'tot_hi_cred_lim' : test_data.tot_hi_cred_lim.tolist(),
    'tot_cur_bal' : test_data.tot_cur_bal.tolist(),
    'total_bal_ex_mort' : test_data.total_bal_ex_mort.tolist()
    })
pd.DataFrame(test_data).fillna(0, inplace=True)
X_test = test_data[colnames_test]
# we choose those features based on it is really neccessary or not.
colnames = ['loan_amnt','term','grade','emp_length','home_ownership','annual_inc','tot_hi_cred_lim','verification_status','dti','pub_rec','loan_status','tot_cur_bal','pct_tl_nvr_dlq','num_tl_op_past_12m','total_bal_ex_mort']

data = pd.read_csv('loansTrain.csv', usecols=colnames)

data['term'] = data['term'].str.extract('(\d+)', expand=False)

data['emp_length'] = data['emp_length'].replace('< 1 year', 0.5)
data['emp_length'] = data['emp_length'].replace('1 year', 1)
data['emp_length'] = data['emp_length'].replace('2 years', 2)
data['emp_length'] = data['emp_length'].replace('3 years', 3)
data['emp_length'] = data['emp_length'].replace('4 years', 4)
data['emp_length'] = data['emp_length'].replace('5 years', 5)
data['emp_length'] = data['emp_length'].replace('6 years', 6)
data['emp_length'] = data['emp_length'].replace('7 years', 7)
data['emp_length'] = data['emp_length'].replace('8 years', 8)
data['emp_length'] = data['emp_length'].replace('9 years', 9)
data['emp_length'] = data['emp_length'].replace('10 years', 10)
data['emp_length'] = data['emp_length'].replace('10+ years', 10.5)

data['home_ownership'] = data['home_ownership'].replace('RENT', 1)
data['home_ownership'] = data['home_ownership'].replace('OWN', 2)
data['home_ownership'] = data['home_ownership'].replace('MORTGAGE', 3)
data['home_ownership'] = data['home_ownership'].replace('ANY', 4)
data['home_ownership'] = data['home_ownership'].replace('OTHER', 5)
data['home_ownership'] = data['home_ownership'].replace('NONE', 6)

data['verification_status'] = data['verification_status'].replace('Verified',1)
data['verification_status'] = data['verification_status'].replace('Not Verified',2)
data['verification_status'] = data['verification_status'].replace('Source Verified',3)

data['grade'] = data['grade'].apply(ord)
#data frame for training.
data = pd.DataFrame({
    'loan_amnt' : data.loan_amnt.tolist(),
    'term' : data.term.tolist(),
    'grade' : data.grade.tolist(),
    'emp_length' : data.emp_length.tolist(),
    'home_ownership' : data.home_ownership.tolist(),
    'annual_inc' : data.annual_inc.tolist(),
    'verification_status' : data.verification_status.tolist(),
    'dti' : data.dti.tolist(),
    'loan_status' : data.loan_status.tolist(),
    'pct_tl_nvr_dlq' : data.pct_tl_nvr_dlq.tolist(),
    'num_tl_op_past_12m' : data.num_tl_op_past_12m.tolist(),
    'pub_rec' : data.pub_rec.tolist(),
    'tot_hi_cred_lim' : data.tot_hi_cred_lim.tolist(),
    'tot_cur_bal' : data.tot_cur_bal.tolist(),
    'total_bal_ex_mort' : data.total_bal_ex_mort.tolist()
    })   

pd.DataFrame(data).fillna(0, inplace=True)
X = data[['loan_amnt','term','grade','emp_length','home_ownership','annual_inc','tot_hi_cred_lim','verification_status','dti','pub_rec','pct_tl_nvr_dlq','num_tl_op_past_12m','tot_cur_bal','total_bal_ex_mort']]
y = data['loan_status']

#we did this because we want to create a validation set. Thats how we measure our results.
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
# those hyper parameters based on some research on internet. #href:https://towardsdatascience.com/optimizing-hyperparameters-in-random-forest-classification-ec7741f9d3f6
# we tried all of them take the best one.
clf=RandomForestClassifier(random_state = 1,
                                  n_estimators = 750,
                                  max_depth = 15, 
                                  min_samples_split = 5,  min_samples_leaf = 1)
# we train our model with whole training data.
clf.fit(X,y)
# we predict those and ...
y_pred=clf.predict(X_test)
'''
#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
# Model Accuracy, how often is the classifier correct?
print("Accuracy:", metrics.accuracy_score(y_test, y_pred))
'''
# ... write on the submission file which we submit on kaggle.
id_num = 0
with open('submission.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(["ID", "loan_status"])
    for prediction in y_pred:
      writer.writerow([id_num, prediction])
      id_num += 1

from google.colab import drive
drive.mount('/content/drive')

"""First, we tried to select the features from the list that could actually increase the reliability of the results of our project. We searched for the parameters that could affect someone's loan status. In the beginning, we selected a few of the given features; as we added new ones, we saw that our accuracy increased more and more. But, as we saw in the lectures, after some point, adding several features could harm your system’s performance. Therefore, we tried to avoid the features that actually end up in the similar meaning; for instance, employment length and employment title. One needs to know whether the applicant for the loan is working or not; and it can be understand from the job title and from employment length too. What we mean by that is if the job title is empty, then the applicant is unemployed which is actually same as an empty job length. Therefore, we just used one of them with the other features such as grade, home ownership, annual income, dti, public record and so on. In order to work in an efficient way with our data, we replaced the chosen features with related integers. Moreover, we split our data into training and validation data with 20% ratio. And, we saw 0.803 accuracy in our validation data. However, after testing our system with the test data, we got 0.78 accuracy. Therefore, we tried to find other ways to increase our accuracy. We found out that making changes on hyper parameters of random forest which was the model we were using, could affect our accuracy. After doing some research, we found out with trial and error that doing changes on hyper parameters such as random state, n_estimators, max_depth, min_samples_split and min_samples_leaf increased our accuracy to 0.801.

Those features and hyper parameters result => Accuracy: 0.80142
"""